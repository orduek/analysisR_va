---
title: "Analyses for Network approach paper"
author: "Or Duek & Tobias Spiller"
output:
html_document:
df_print: paged
---
### This is an *Analysis* script for network approach analysis of PCL and PCL + PHQ9 networks. Cohort of VA/DoD administrative data set. 

# Version 0.9.1 21.04.2020 - OAD
  
## Table of Contents
1.  Load libraries 
2.  Import and prepare data  & descriptives
3.  Compare different estimation techniques
3.1  PCL Network
3.2  PCL & PHQ9 Network
4. Estiamte Networks incl. centrality and stability analyses
4.1 PCL Network 
4.2 PCL & PHQ9 Network
5. Confirmatory Network Analysis
5.1 PCL Network
5.2 PCL & PHQ9 Network
6. Community Analysis
7. Predictability
8. Session info
9. Open Questions

<br/><br/>
<br/><br/>
## 1. Load libraries
```{r}
# Data handeling
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("corrplot")) install.packages("corrplot")  ## correlation matrix plots
# if(!require("OpenMX")) install.packages("OpenMx") 
# 
# # Network packages
 if(!require("qgraph")) install.packages("qgraph")
 if(!require("psychonetrics")) install.packages("psychonetrics")
 if(!require("bootnet")) install.packages("bootnet")
 if(!require("mgm")) install.packages("mgm")
 if(!require("networktools")) install.packages("networktools")
 if(!require("EGAnet")) install.packages("EGAnet")

# devtools::install_github("donaldRwilliams/BGGM", force = TRUE)
# devtools::install_github("donaldRwilliams/GGMnonreg", force = TRUE)
library(BGGM)
library(GGMnonreg)

require("tidyverse")
require("corrplot")
library(OpenMx)
library(devtools)
require(lavaan)
require(ltm)
```
 
## 2. Import and prepare data  & descriptives
```{r}
# load data set
source('/home/or/Documents/va_data/readData.r')
```

#### Data cleanning and descriptive statistics
```{r}
# Addind time difference between PCL and PHQ
# gather info on both meds and no meda

# all patientes with PTSD and PCLTOT
pclAll <- dplyr::filter(vaDatclean, !is.na(BPCLTOT))
# plot pcl total score 
hist(pclAll$BPCLTOT)
# we have a minimum of 2 - so we have some NAs - let remove them
pclAll_Nas <- filter(pclAll, BPCLTOT <=16)
# total of 20 subjects with 16 or less in PCL (i.e. at least one missing variable)
# we can remove them from analysis
pclAll <- filter(pclAll, BPCLTOT >=17)
# 159577 patients
#pclNetwork <- pclNoNa # just medicated
pclNetwork <- pclAll
nrow(pclNetwork)
hist(pclNetwork$BPCLTOT)

pclNetwork$PCL_PHQdiff <- pclNetwork$PHQSURVEYDATE - pclNetwork$PCLSURVEYDATE
pclPHQNetwork <- filter(pclNetwork, PCL_PHQdiff <= 14 & PCL_PHQdiff >= 0) # removing patients with more than 14 apart between PHQ9 and PCL-M
pclPHQNetwork <- filter(pclPHQNetwork, BPCLTOT>=17)
hist(pclPHQNetwork$PCL_PHQdiff)
```

#### Sample descriptives of all subjects
```{r sample descriptives}
# gather info on both meds and no meda
# remove patients with more than 14 days apart PHQ and PCL
pclAll <- filter(pclAll, AGE_OCT01<110)
summary(pclAll$AGE_OCT01)
mean(pclAll$AGE_OCT01, na.rm=TRUE)
sd(pclAll$AGE_OCT01, na.rm=TRUE)
summary(pclAll$BPCLTOT)
mean(pclAll$BPCLTOT)
sd(pclAll$BPCLTOT)
table(pclAll$FEMALE)
summary(pclPHQNetwork$PHQSUMTOTAL)
mean(pclPHQNetwork$PHQSUMTOTAL, na.rm=TRUE)
sd(pclPHQNetwork$PHQSUMTOTAL, na.rm=TRUE)
table(pclPHQNetwork$FEMALE)

```

## Compare these demorgraphics with the rest of the sample

```{r}
noPCL <- dplyr::filter(vaDatclean, is.na(BPCLTOT))
summary(noPCL$AGE_OCT01)
noPCL <- filter(noPCL, AGE_OCT01<110)
hist(noPCL$AGE_OCT01)
mean(noPCL$AGE_OCT01, na.rm=TRUE)
sd(noPCL$AGE_OCT01, na.rm=TRUE)
t.test(noPCL$AGE_OCT01, pclAll$AGE_OCT01)
table(noPCL$FEMALE)

chisq.test(rbind(table(noPCL$FEMALE), table(pclAll$FEMALE)))
```


```{r}
# build data set only with PCL items
pclItems <- dplyr::select(pclAll, starts_with("PCL"))
pclPHQItems <- dplyr::select(pclPHQNetwork, starts_with("PCL"))

pclItems_noCluster <- dplyr::select(pclItems, -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE)
nrow(pclItems_noCluster)
pclItems_noCluster <- na.omit(pclItems_noCluster)
nrow(pclItems_noCluster)
## alpha cronbach
cronbach.alpha(pclItems_noCluster, CI = T)

pclPHQNetwork_noCluster <- dplyr::select(pclPHQNetwork, starts_with("PCL"), -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE, -PCL_PHQdiff, PHQ1,PHQ2,PHQ3, PHQ4,PHQ5,PHQ6,PHQ7,PHQ8,PHQ9)
pclPHQNetwork_noCluster <- na.omit(pclPHQNetwork_noCluster)

## Alpha cronbach
cronbach.alpha(pclPHQNetwork_noCluster)
```

### Compare PCL distribution between the two samples:

```{r}
MASS::truehist(pclAll$BPCLTOT)
summary(pclAll$BPCLTOT, rm.na = TRUE)

summary(pclPHQNetwork$BPCLTOT, rm.na=TRUE)
sd(pclPHQNetwork$BPCLTOT)
```

## 3.  Compare different estimation techniques 
### 3.1  PCL Network
<br/><br/>
#### We compare several models to set the one's we would use thorugh the rest of analysis
#### First we do it on the PCL items alone (whole 150k)
```{r}


### A. Gaussian Graphical Model, regularized
df2 <- pclItems_noCluster

# Define labels
labels <- names(df2)

n2 <- estimateNetwork(df2, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
g2 <- plot(n2, legend.cex=.5, vsize=7)
# Severely skewed data, so we use Spearman over polychoric correlations here, as recommended (https://psycnet.apa.org/record/2018-13501-001)
# Warning: Dense network selected. But no negative edges, and bootstrapped edge weights look OK.
# However, we can still try threshold=TRUE as recommended; see next.

### B. Gaussian Graphical Model, regularized & thresholded
n3 <- estimateNetwork(df2, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=TRUE)
g3 <- plot(n3, layout=g2$layout, legend.cex=.5, vsize=7)

### C. Robustness: use new estimation procedure ggmModSelect - this is a nonregularized network (http://psychosystems.org/qgraph_1.5)
n4 <- estimateNetwork(df2, default="ggmModSelect", corMethod = "cor", corArgs = list(method="spearman"))
g4 <- plot(n4, layout=g2$layout, legend.cex=.5, vsize=7)

### D.1 Nonregularized network - I've used this one simply enough - please comment on better methods
n5 <- GGM_regression(df2,IC = "BIC", method= "forward")
g5 <- qgraph(n5$pcor_selected, layout = g2$layout, theme = "colorblind", labels = labels)

### D.2 - TOBIAS' SUGGESTION - Nonregularized network
n5_2 <-GGM_bootstrap(df2, alpha = 0.05, sims = 1000)
g5_2 <- qgraph(n5_2$pcor_selected, layout = g2$layout, theme = "colorblind", labels = labels)

### E Bayesian Network
n6 <- explore(df2, prior_sd = 0.5, iter = 5000, cores = 6)
# H1 for this network is, that edges are not zero (either negative or positive), H0 is that they are zero #
E <- BGGM::select(n6, BF_cut = 10, alternative = "two.sided")
# Summary table of the network ##
summary(E, summarize = T, log = TRUE, digits = 2)  # log TRUE: BF= ln(BF), BF_10 = evidence for H1, BF_01 = evidence for H0
# Plot
g6 <- qgraph(E$pcor_mat, legend = FALSE, layout = g2$layout, theme = "colorblind", labels = labels)


```

#### Correlate the different networks with the first network (EBIC, spearman) to check similarity
```{r}
cor(vechs(n2$graph), vechs(n4$graph)) # 0.997
cor(vechs(n2$graph), vechs(n4$graph), method="spearman") # 0.993
cor(vechs(n2$graph), vechs(n3$graph), method="spearman") #0.99
cor(vechs(n4$graph), vechs(n3$graph), method="spearman") #0.989
cor(vechs(n5$pcor_or), vechs(n2$graph), method = "spearman") #0.958
cor(vechs(n5_2$mat_mean), vechs(n2$graph), method = "spearman") #
cor(vechs(E$partials_non_zero), vechs(n2$graph), method = "spearman") #0.95

# Looks like all networks are highly correlated with each other. 
# We choose n2 from here on, although looks like all are very similar - do you suggest different network to go with? TOBIAS: I think it doesn't matter
```
### Look at edges in n4
```{r}
m <- n4_phq$graph
top_frac(as.data.frame(m), 5)
# get top 5%
m[m > quantile(m, prob=0.95)]
idx = which(m > quantile(m, prob=0.95), arr.ind=TRUE)
b <- m[idx]
sort(b)

```

#### Estiamte Predictability (using mgm) - TOBIAS' SUGGESTION
```{r}
fit1 <- mgm(data = na.omit(df2), type = rep('g', 17), lambdaSel = 'CV', level = rep(1,17), k = 3)
pred1 <- predict(fit1, na.omit(df2), errorCon = "R2")

pred1$errors #list with predictability for each node

# Average node predictability #
R2_1<-as.numeric(pred1$errors$R2)
mean(R2_1) # 0.464


# ## Plot network and save as PDF - for now plotting here only. 
# pdf("PCL_Network_predictab.pdf", width=10, height=10)
g2_mgm <- qgraph(n4$graph, pie=R2_1, title="PCL-Network", legend.cex=.5, vsize=7, layout=g2$layout, theme= "colorblind")
#dev.off()
```

### 3.2  PCL & PHQ9 Network
#### Same procedure for combined PCL and PHQ9 network
```{r}
### A. Gaussian Graphical Model, regularized
df3 <- pclPHQNetwork_noCluster

labels_2 <- names(df3)

n2_phq <- estimateNetwork(df3, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
g2_phq <- plot(n2_phq, legend.cex=.5, vsize=7)

### B. Gaussian Graphical Model, regularized & thresholded
n3_phq <- estimateNetwork(df3, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=TRUE)
g3_phq <- plot(n3_phq, layout=g2_phq$layout, legend.cex=.5, vsize=7)
# Network too sparse, enforcing high specificity at the cost of sensitivity not reasonable here.

### C. Robustness: use new estimation procedure ggmModSelect (http://psychosystems.org/qgraph_1.5)
n4_phq <- estimateNetwork(df3, default="ggmModSelect", corMethod = "cor", corArgs = list(method="spearman"))
g4_phq <- plot(n4_phq, layout=g2_phq$layout, legend.cex=.5, vsize=7)

### D.1 Nonregularized network
n5_phq <- GGMregress(df3,IC = "BIC", method= "forward")
g5_phq <- qgraph(n5_phq$pcor_or, layout = g2_phq$layout, theme = "colorblind", labels = labels_2)

### D.2 - TOBIAS' SUGGESTION - Nonregularized network

n5_2_phq <- GGMnonreg::GGMboot(df3, alpha = 0.05, sims = 1000)
g5_2_phq <- qgraph(n5_2_phq$mat_mean, layout = g2_phq$layout, theme = "colorblind", labels = labels_2)



### E Bayesian Network
n6_phq <- explore(df3, prior_sd = 0.5, iter = 5000, cores = 4)
# H1 for this network is, that edges are not zero (either negative or positive), H0 is that they are zero #
E_phq <- BGGM::select(n6_phq, BF_cut = 10, alternative = "two.sided")
# Summary table of the network ##
summary(E_phq, summarize = T, log = TRUE, digits = 2)  # log TRUE: BF= ln(BF), BF_10 = evidence for H1, BF_01 = evidence for H0
# Plot
g6 <- qgraph(E_phq$partials_non_zero, legend = FALSE, layout = g2_phq$layout, theme = "colorblind", labels = labels_2)

```

#### Estiamte Predictability (using mgm) for PCL & PHQ9

```{r}
fit2 <- mgm(data = na.omit(df3), type = rep('g', 26), lambdaSel = 'CV', level = rep(1,26), k = 3)
pred2 <- predict(fit2, na.omit(df3), errorCon = "R2")

pred2$errors #list with predcitability for each node

# Average node predictability #
R2_2<-as.numeric(pred2$errors$R2)
mean(R2_2) # 0.467

# sort predictability
view(sort(pred2$errors))

# ## Plot network and save as PDF - for now plotting here only. 
# pdf("PCL_Network_predictab.pdf", width=10, height=10)
g3 <- plot(n4_phq, pie=R2_2, title="PCL&PHQ9-Network", legend.cex=.5, vsize=7)
#dev.off()
```
#### Correlate the different networks with the first network (EBIC, spearman) to check similarity
```{r}
cor(vechs(n2_phq$graph), vechs(n4_phq$graph)) # 0.995
cor(vechs(n2_phq$graph), vechs(n4_phq$graph), method="spearman") # 0.973
cor(vechs(n2_phq$graph), vechs(n3_phq$graph), method="spearman") #0.92
cor(vechs(n4_phq$graph), vechs(n3_phq$graph), method="spearman") #0.94
cor(vechs(n5_phq$pcor_or), vechs(n2_phq$graph), method = "spearman") #0.958
cor(vechs(E_phq$partials_non_zero), vechs(n2_phq$graph), method = "spearman") #0.948


# We choose n2_phq from here on - should we go with a different one?  #TOBIAS: I think it doesn't matter
```

#### Check nodes for high correlation with each other
```{r}
# run goldbricker to test correlation between nodes
gold <- goldbricker(df3)
print(gold$suggested_reductions)
print(gold$proportion_matrix)
# If I'm doing it right - no suggested reductions

# just want to see wat's going on
cor.test(df3$PCL13, df3$PHQ3) # highly correlated

cor.test(df3$PCL15, df3$PHQ7)
```

<br/><br/>
<br/><br/>
## 4. Estiamte Networks incl. centrality and stability analyses
### 4.1 PCL Network
```{r network_pcl}
# building two kinds of group clusters. One (gr1) taken from Harpaz-Rotem, I., Tsai, J., Pietrzak, R. H., & Hoff, R. (2014).
gr1 <- list("Re-experiencing"=c(1:5), "Avoidance"=c(6:7), "Emotional numbing"=c(8:12),"Dysphoric arousal"=c(13:15), "Anxious arousal"=c(16:17)) #PTSD Clusters
gr_likeDSM <- list("Intrusion"=c(1:5), "Avoidance"=c(6:12), "Arousal"=c(13:17)) #PTSD symptoms categories B C D

g2 <- plot(n2, pie=R2_1, legend.cex=.5, vsize=7, theme = "colorblind")#, groups = gr_likeDSM)

# Currently no predictability analysis. We should discuss if we want to include it
```

#### Centrality graph of PCL network:
```{r}
# sort by level of centrality (strength)
sort(centrality(n4)$InExpectedInfluence  , decreasing = T)
# plot centrality
centralityPlot(n4, include = "all")
```

#### Stability of PCL network
```{r}
# Bootstrap 1:
boot1 <- bootnet(n4, nCores = 10, nBoots = 1000, type = "nonparametric") 
plot(boot1, labels = F, order = "sample")  + theme_minimal()
# now lets look at subjects
boot2 <- bootnet(n4, nCores = 10, nBoots = 1000, type = "case")
plot(boot2, labels = F, order = "sample") + theme_minimal()

# Measure Centrality stability for EI - to check later for differences
boot3 <- bootnet(n4, nCores = 10, nBoots = 1000, statistics = "expectedInfluence", type = "nonparametric")

# plot EI differences
plot(boot3, statistics = "expectedInfluence", plot = "difference")#, labels = F, order = "sample") + theme_minimal()

#plot sig diff edges
plot(boot1, "edge", plot = "difference", onlyNonZero = TRUE, order = "sample")
```

### 4.1 PCL & PHQ9 Network
```{r}
#grep("PHQ", colnames(pclPHQNetwork_noCluster)) # get location of PHQ
gr2 <- list("Re-experiencing"=c(1:5), "Avoidance"=c(6:7), "Emotional numbing"=c(8:12),"Dysphoric arousal"=c(13:15), "Anxious arousal"=c(16:17), "PHQ9-Depression"=c(18:26)) #PTSD symptoms categories B C D E  
plot(n4_phq, pie=R2_2, theme="colorblind")
```

#### Centrality graph of PCL & PHQ9 network
```{r}
# sort by level of centrality (EI)
sort(centrality(n4_phq)$InExpectedInfluence, decreasing = T)
# plot centrality
centralityPlot(n4_phq, include = "all")
```

#### Stability of PCL & PHQ9 network
```{r}
# Bootstrap 1:
boot1_phq <- bootnet(n4_phq, nCores = 11, nBoots = 500, type = "nonparametric") 
plot(boot1_phq, labels = F, order = "sample")  + theme_minimal()
# now lets look at subjects
boot2_phq <- bootnet(n4_phq, nCores = 10, nBoots = 1000, type = "case")
plot(boot2_phq, labels = F, order = "sample") + theme_minimal()


# Measure Centrality stability for EI - to check later for differences
boot3_phq <- bootnet(n4_phq, nCores = 10, nBoots = 1000, statistics = "expectedInfluence", type = "nonparametric")

#plot sig diff edges
plot(boot1_phq, "edge", plot = "difference", onlyNonZero = TRUE, order = "sample")

## Plot sig EI
plot(boot3_phq, "expectedInfluence", plot = "difference", order = "sample")

```



#### Centrality of both networks in a single graph
```{r}
centralityPlot(list("Only PCL" = n2, "PCL_PHQ9" = n2_phq), include = c("ExpectedInfluence"), decreasing = TRUE) + theme_minimal() 
## plotting EI and predictability for each
# first build a data frame with node names, EI and predictability
# for PCL only
pclOnly_EI <- centrality(n2)$InExpectedInfluence
pclOnly_R2_1 <- R2_1
labels
pcl_only <- data_frame(labels, pclOnly_EI, pclOnly_R2_1) 
pcl_only <- rename(pcl_only, ExpectedInfluence = pclOnly_EI, Predictability = pclOnly_R2_1)
# make labels an ordered factor
pcl_only$labels <- factor(pcl_only$labels, ordered = TRUE, levels = labels)
pcl_only_long <- pivot_longer(pcl_only, cols = c(ExpectedInfluence, Predictability))
ggplot(pcl_only_long, aes(x=labels, y=value, group=name)) + geom_point() + geom_line() + facet_grid(.~name) + theme_minimal() + ylab("EI/R^2") + xlab("node") + coord_flip()


## do the same for pcl+phq9
pclPhq_EI <- centrality(n2_phq)$InExpectedInfluence
pclPhq_R2_2 <- R2_2
labels_2
pclPhq <- data_frame(labels_2, pclPhq_EI, pclPhq_R2_2)
pclPhq <- rename(pclPhq,ExpectedInfluence = pclPhq_EI, Predictability = pclPhq_R2_2, labels = labels_2)
pclPhq$labels <- factor(pclPhq$labels, ordered = TRUE, levels = labels_2)
pclPhq_long <- pivot_longer(pclPhq, cols = c(ExpectedInfluence, Predictability))
ggplot(pclPhq_long, aes(x=labels, y=value, group = name)) + geom_point() + geom_line() + facet_grid(.~name)+ theme_minimal() + ylab("EI/R^2") + xlab("Node") + coord_flip()

```
### Correlation between EI and predictability of those two

```{r}
cor.test(R2_1, R2_2[1:17]) # 0.85


pcl_only_long$group = 'PTSD Only'
pclPhq_long$group = 'PTSD & Depressive Symptoms'
pcl_both <- rbind(pcl_only_long, pclPhq_long)
# plot the two toghether. 

ggplot(pcl_both, aes(x=labels, y=value, color=group, group=group)) + geom_point() + geom_line() + facet_grid(.~name) + theme_minimal() + ylab("EI/R^2") + xlab("Node") + coord_flip()

```

## 5. Confirmatory network analysis
#### We randomly sample 50% of population and run the model, then fit on the other 50%
<br/><br/>
<br/><br/>
### 5.1 PCL Network
#### Split Data into Training and Testing in R 
```{r}
sample_size = floor(0.5*nrow(pclItems_noCluster))
set.seed(777)

# randomly split data in r
picked = sample(seq_len(nrow(pclItems_noCluster)),size = sample_size)
train =pclItems_noCluster[picked,]
test =pclItems_noCluster[-picked,]

# Start run confirmatory analysis
# run model on half the subjects 
# shuold we consider comparing fit of different sets of models? or is it too much?
#net <-  estimateNetwork(train, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
net <- estimateNetwork(train, default = "ggmModSelect", verbose = FALSE)
network <- 1*(net$graph != 0)
model_frombootnet <- ggm(train, omega = network) %>% runmodel
```

#### Run analysis
```{r}
adjacency <- network #1*(getmatrix(model_frombootnet, "omega")!=0)
confirmatory <- ggm(test, omega = adjacency)
confirmatory <- confirmatory %>% runmodel

confirmatory %>% fit
```
```{r}
# saving to word table
library(rtf)
rtffile <- RTF("fitResults.doc")  # this can be an .rtf or a .doc
addParagraph(rtffile, "This is the output of fit results:\n")
addTable(rtffile, as.data.frame((confirmatory %>% fit)))
done(rtffile)
```

#### Compare fit indices
```{r}
compare(train = model_frombootnet , test = confirmatory)
```

### 5.2 PCL & PHQ9 Network
#### Running confirmatory analysis on PCL and PHQ9
```{r}
pclPHQNetwork_noCluster

# Split Data into Training and Testing in R 
sample_sizePHQ = floor(0.5*nrow(pclPHQNetwork_noCluster))
set.seed(777)

# randomly split data in r
pickedPHQ = sample(seq_len(nrow(pclPHQNetwork_noCluster)),size = sample_sizePHQ)
trainPHQ =pclPHQNetwork_noCluster[pickedPHQ,]
testPHQ =pclPHQNetwork_noCluster[-pickedPHQ,]

# Start run confirmatory analysis
require(psychonetrics)
# run model on half the subjects 
#netPHQ <- estimateNetwork(trainPHQ, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
netPHQ <- estimateNetwork(trainPHQ, default = "ggmModSelect", verbose = FALSE)
networkPHQ <- 1*(netPHQ$graph != 0)
model_frombootnetPHQ <- ggm(trainPHQ, omega = networkPHQ) %>% runmodel
```

#### Run analysis
```{r}
# confirmatory 
adjacencyPHQ <- networkPHQ # fixed according to Eiko's comment
confirmatoryPHQ <- ggm(testPHQ, omega = adjacencyPHQ)
confirmatoryPHQ <- confirmatoryPHQ %>% runmodel

confirmatoryPHQ %>% fit
```

```{r}
rtffile_phq <- RTF("fitResults_phq.doc")  # this can be an .rtf or a .doc
addParagraph(rtffile_phq, "This is the output of fit results for PHQ and PCL:\n")
addTable(rtffile_phq, as.data.frame((confirmatoryPHQ %>% fit)))
done(rtffile_phq)
```

#### Compare fit indices
```{r}
compare(train = model_frombootnetPHQ , test = confirmatoryPHQ)
```

## 6. Community Analysis
#### Data driven clustering using EGA
```{r}
# Cluster of PCL and PHQ
egaLasso<-EGA(df3, plot.EGA = TRUE, steps = 4, model = "glasso")
egaTMFG <- EGA(df3, plot.EGA = TRUE, steps = 4, model = "TMFG")


# run CFA
pcl_phqCFA <- CFA(ega.obj = egaLasso,data = df3,plot.CFA = TRUE, estimator = "WLSMV")
# bootstrap
bootEGA_pclPHQ <- bootEGA(df3, 500, type = "resampling", ncores = 5)

# Cluster only PCL
egaPCL_glasso<-EGA(df2, plot.EGA = TRUE, steps = 4,  model = "glasso")
egaPCL_TMFG<-EGA(df2, plot.EGA = TRUE, steps = 4,  model = "TMFG")
# plot ega gLASSO
plot(egaPCL_glasso, theme = "colorblind", layout =g2$layout)

pclCFA_glasso <- CFA(ega.obj = egaPCL_glasso,data = df2,plot.CFA = TRUE, estimator = "WLSMV")
# plot CFA gLASSO
plot(pclCFA_glasso, theme="colorblind")

pclCFA_TMFG <- CFA(ega.obj = egaPCL_TMFG,data = df2,plot.CFA = TRUE, estimator = "WLSMV")

bootEGA_pcl <- bootEGA(df2, 500, type = "resampling", ncores = 5)
plot(bootEGA_pcl)
# bootstrap results in the same structure
# run cfa on 5 factor model
```
### Only PCL has three clusters (when EGA load is set to 0.7 )
## We should compare the confirmatory analysis of PTSD (PCL) based on the EGA with one based on the 5-factor model

```{r}
# gr1 <- list("Re-experiencing"=c(1:5), "Avoidance"=c(6:7), "Emotional numbing"=c(8:12),"Dysphoric arousal"=c(13:15), "Anxious arousal"=c(16:17))
model_5Factor <- ' ReExperiencing =~ PCL1 + PCL2 + PCL3 + PCL4 + PCL5
Avoidance =~ PCL6 + PCL7
Numbing =~ PCL8 + PCL9 + PCL10 + PCL11 + PCL12
DysphoricArousal =~ PCL13 + PCL14 + PCL15
AnxiousArousal =~ PCL16 + PCL17 '
theoModel <- cfa(model_5Factor, data = df2, estimator = "WLSMV")

# fit measures of 5 factor model
fitMeasures(theoModel, c("chisq","df","pvalue","srmr","cfi","rmsea"))
# fit measures of EGA
fitMeasures(pclCFA_glasso$fit, c("chisq","df","pvalue","srmr","cfi","rmsea"))
# seems like 5 models has a bit better fit
fitMeasures(pclCFA_TMFG$fit, c("chisq","df","pvalue","srmr","cfi","rmsea"))

# seems like TMGF is the worst option

# Lets run Chisq to compare each model

lavTestLRT(theoModel,pclCFA_TMFG$fit)
lavTestLRT(theoModel,pclCFA_glasso$fit)
```
## Lets plot the theoretical modeling, just for the sake of plotting
```{r}
semPlot::semPaths(theoModel, what = "est", layout = "spring", theme = "colorblind")#, title = FALSE, curvePivot = TRUE)
```
## We can next compare it with the DSM-IV model (3 factors)
```{r}
#gr_likeDSM <- list("Intrusion"=c(1:5), "Avoidance"=c(6:12), "Arousal"=c(13:17)) #PTSD symptoms categories B C D
dsm_Model <- 'Intrusion =~ PCL1 + PCL2 + PCL3 + PCL4 + PCL5 
Avoidance =~ PCL6 + PCL7 + PCL8 + PCL9 + PCL10 + PCL11 + PCL12
Arousal =~ PCL13 + PCL14 + PCL15 + PCL16 + PCL17'

DSMModel <- cfa(dsm_Model, data = df2, estimator = "WLSMV")
fitMeasures(DSMModel, c("chisq","df","pvalue","srmr","cfi","rmsea"))
lavTestLRT(theoModel,DSMModel)
lavTestLRT(DSMModel,pclCFA_glasso$fit, method ="satorra.2000")
```
5-factor model outperform theoretical model. 


## Compare the PTSD and the comorbidity network (n2 and n2_phq)
```{r}
deltaNet <- n2$graph - n2_phq$graph[1:17,1:17] 
qgraph(deltaNet, layout = g2$layout, theme = "colorblind", threshold = 0.03)#, labels = labels)
max(deltaNet)
min(deltaNet)
mean(abs(deltaNet[upper.tri(deltaNet,diag=FALSE)]))
cor(as.vector(n2$graph),as.vector(n2_phq$graph[1:17,1:17]), method="spearman") #0.98 very high correlation between the networks. 

###


sum1<-sum(abs(n2$graph[upper.tri(n2$graph,diag=FALSE)])); sum1     # 7.9; sum/2 of ptsd network matrix
mean1<-mean(abs(n2$graph[upper.tri(n2$graph,diag=FALSE)])); mean1  # 0.05; mean edge strength
sum2<-sum(abs(n2_phq$graph[1:17,1:17][upper.tri(n2_phq$graph[1:17,1:17],diag=FALSE)])); sum2             # 7.37; sum/2 of ptsd + MDD matrix, with mdd cells deleted (ptsd network accounting for mdd symptoms but without them)
mean2<-mean(abs(n2_phq$graph[1:17,1:17][upper.tri(n2_phq$graph[1:17,1:17],diag=FALSE)])); mean2          # 0.05; mean edge strength
1-sum2/sum1 # change in connectivity of PTSD network once symptoms are added 7.2%%


centralityPlot(deltaNet, include = c("ExpectedInfluence"), decreasing = F)
# Measure Centrality stability for EI - to check later for differences


NCT(n2$graph, n2_phq$graph[1:17,1:17] ,weighted = T, test.edges = T, test.centrality = T, it=500)
```
### One last thing (?)
Lets take the PHQ group and split them using cut off (8-11) according to:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3281183/
Lets see if these network are any different

```{r}
pclPHQNetwork_noCluster

summary(pclPHQNetwork$PHQSUMTOTAL)
pclPHQNetwork$MDD <- ifelse(pclPHQNetwork$PHQSUMTOTAL >=11,  'MDD', 'noMDD')

## create two datasets
dfMDD <- filter(pclPHQNetwork, MDD=='MDD')
## 24,239 patients crossed the MDD threshold of 11
dfNoMDD <- filter(pclPHQNetwork, MDD=='noMDD')
## 9035 patients with no MDD

## Now lets create two distinct networks and see if they look the same or not
dfMDD_noCluster <- dplyr::select(dfMDD, starts_with("PCL"), -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE, -PCL_PHQdiff, PHQ1,PHQ2,PHQ3, PHQ4,PHQ5,PHQ6,PHQ7,PHQ8,PHQ9, -MDD)
dfMDD_noCluster <- na.omit(dfMDD_noCluster)

dfNoMDD_noCluster <- dplyr::select(dfNoMDD, starts_with("PCL"), -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE, -PCL_PHQdiff, PHQ1,PHQ2,PHQ3, PHQ4,PHQ5,PHQ6,PHQ7,PHQ8,PHQ9, -MDD)
dfNoMDD_noCluster <- na.omit(dfNoMDD_noCluster)

# now we can estimate a netowork for each
mddNet <- estimateNetwork(dfMDD_noCluster, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
mddGraph <- plot(mddNet, legend.cex=.5, vsize=7)

## No mdd
nomddNet <- estimateNetwork(dfNoMDD_noCluster, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
nomddGraph <- plot(nomddNet, legend.cex=.5, vsize=7)


centralityPlot(list("MDD Network" = mddNet, "No MDD" = nomddNet), include = c("ExpectedInfluence"), decreasing = TRUE) + theme_minimal() 

##
# sort by level of centrality (EI)
sort(centrality(mddNet)$InExpectedInfluence, decreasing = T)

sort(centrality(nomddNet)$InExpectedInfluence, decreasing = T)
# compare networks
compare <- NCT(mddNet, nomddNet ,weighted = T, test.edges = T, test.centrality = T, it=500)
compare$glstrinv.pval # no difference in global strength
compare$nwinv.pval # sig. difference in maximum difference in edge weight
sum(compare$einv.pvals$`p-value` < 0.05) # 61 edges significantly different from each other. 
compare$diffcen.pval # PCL15, PCL14, PCL13, PCL12, PCL8, PCL6, PCL5, PCL3, PCL1 are all significantly different in their centrlity between these two networks. 
compare$diffcen.real
```
## Run same analysis with same two groups, but only on PCL

```{r}
## Now lets create two distinct networks and see if they look the same or not
dfMDD_noClusterPCL <- dplyr::select(dfMDD, starts_with("PCL"), -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE, -PCL_PHQdiff, -MDD)
dfMDD_noClusterPCL <- na.omit(dfMDD_noClusterPCL)

dfNoMDD_noClusterPCL <- dplyr::select(dfNoMDD, starts_with("PCL"), -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE, -PCL_PHQdiff, -MDD)
dfNoMDD_noClusterPCL <- na.omit(dfNoMDD_noClusterPCL)

# now we can estimate a netowork for each
mddNet_pcl <- estimateNetwork(dfMDD_noClusterPCL, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
mddGraph_pcl <- plot(mddNet_pcl, legend.cex=.5, vsize=7)

## No mdd
nomddNet_pcl <- estimateNetwork(dfNoMDD_noClusterPCL, default="EBICglasso", corMethod = "cor", corArgs = list(method="spearman"), threshold=FALSE)
nomddGraph_pcl <- plot(nomddNet_pcl, legend.cex=.5, vsize=7)


centralityPlot(list("MDD Network" = mddNet_pcl, "No MDD" = nomddNet_pcl), include = c("ExpectedInfluence"), decreasing = TRUE) + theme_minimal() 

##
# sort by level of centrality (EI)
sort(centrality(mddNet_pcl)$InExpectedInfluence, decreasing = T)

sort(centrality(nomddNet_pcl)$InExpectedInfluence, decreasing = T)
# compare networks
compare_pcl <- NCT(mddNet_pcl, nomddNet_pcl ,weighted = T, test.edges = T, test.centrality = T, it=500)
compare_pcl$glstrinv.pval # no difference in global strength
compare_pcl$nwinv.pval # sig. difference in maximum difference in edge weight
sum(compare_pcl$einv.pvals$`p-value` < 0.05) # 30 edges significantly different from each other. 
compare_pcl$diffcen.pval # PCL1, PCL4, PCL5, PCL6, PCL8, PCL13, PCL14, PCL16 are all significantly different in their centrlity between these two networks. 
compare_pcl$diffcen.real

```


## 7. Session info
```{r}
session_info()
```
<br/><br/>
<br/><br/>
# 8. Open Questions
### Open Questions II
1. Should deal with PCL15 and PHQ9-7 (two concentration) , and PCL13 anf PHQ9-3 (two sleep) - please see tests above and comment on that. 
2. *Tobias* at all: Should we include Predictability? I think so
